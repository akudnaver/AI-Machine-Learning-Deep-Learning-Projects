{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akudnaver/AI-Machine-Learning-Deep-Learning-Projects/blob/master/chatbot_pdf_using_gpt2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-07-11T12:45:00.232569Z",
          "iopub.status.busy": "2024-07-11T12:45:00.232206Z",
          "iopub.status.idle": "2024-07-11T12:45:00.237620Z",
          "shell.execute_reply": "2024-07-11T12:45:00.236684Z",
          "shell.execute_reply.started": "2024-07-11T12:45:00.232540Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HxNQO80z98Fm",
        "outputId": "9aef8827-5ea8-4b02-8695-3464c3cdaefd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "/UX_Lifecycle.pdf\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "print(os.getcwd())\n",
        "# file_path = \"/UX_Lifecycle.pdf\"\n",
        "filename =('/UX_Lifecycle.pdf')\n",
        "print(filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-07-11T12:45:00.253624Z",
          "iopub.status.busy": "2024-07-11T12:45:00.253366Z",
          "iopub.status.idle": "2024-07-11T12:45:00.257907Z",
          "shell.execute_reply": "2024-07-11T12:45:00.256997Z",
          "shell.execute_reply.started": "2024-07-11T12:45:00.253602Z"
        },
        "trusted": true,
        "id": "z92w6QPQ98Fp"
      },
      "outputs": [],
      "source": [
        "# !pip install sentence_transformers\n",
        "# !pip install pdfminer.six\n",
        "# !pip install text_generation\n",
        "# !pip install einops --no-deps\n",
        "# !pip install --upgrade torch transformers\n",
        "# !pip install flask\n",
        "# !pip install torchvision\n",
        "# !pip install torchaudio\n",
        "# !pip install --upgrade gradio\n",
        "# !pip install semantic_version\n",
        "# !pip install ffmpy --no-deps\n",
        "# !pip install gradio_client\n",
        "# !pip install multipart\n",
        "# !pip install python-multipart\n",
        "# !pip install tensorflow\n",
        "# !pip install gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-07-11T12:45:00.274214Z",
          "iopub.status.busy": "2024-07-11T12:45:00.273581Z",
          "iopub.status.idle": "2024-07-11T12:45:00.279086Z",
          "shell.execute_reply": "2024-07-11T12:45:00.278173Z",
          "shell.execute_reply.started": "2024-07-11T12:45:00.274191Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7B7xsrGp98Fq",
        "outputId": "6b416f0f-6709-44f0-90b1-01c699c82bcf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPUs: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
            "GPU is available.\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "# List GPUs available\n",
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "print(\"GPUs:\", gpus)\n",
        "\n",
        "# Check if GPUs are available\n",
        "if gpus:\n",
        "    print(\"GPU is available.\")\n",
        "else:\n",
        "    print(\"GPU is not available.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-07-11T12:45:00.293921Z",
          "iopub.status.busy": "2024-07-11T12:45:00.293665Z",
          "iopub.status.idle": "2024-07-11T12:45:00.298433Z",
          "shell.execute_reply": "2024-07-11T12:45:00.297570Z",
          "shell.execute_reply.started": "2024-07-11T12:45:00.293900Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EtZld_r098Fq",
        "outputId": "a80fe9cc-e9ab-4ea3-850f-780bcfbd18f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pydantic/_internal/_fields.py:161: UserWarning: Field \"model_id\" has conflict with protected namespace \"model_\".\n",
            "\n",
            "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import argparse\n",
        "import json\n",
        "import torch\n",
        "from pdfminer.high_level import extract_text\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "from sentence_transformers import SentenceTransformer, CrossEncoder, util\n",
        "from text_generation import Client\n",
        "from flask import Flask, request, jsonify"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-07-11T12:45:00.315342Z",
          "iopub.status.busy": "2024-07-11T12:45:00.315076Z",
          "iopub.status.idle": "2024-07-11T12:45:01.367180Z",
          "shell.execute_reply": "2024-07-11T12:45:01.366181Z",
          "shell.execute_reply.started": "2024-07-11T12:45:00.315321Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "86W3hpDl98Fr",
        "outputId": "f7cc7df0-c0e3-40ff-890d-7ed278e7ded7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/models/auto/tokenization_auto.py:778: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py:469: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "# PREPROMPT = \"Below are a series of dialogues between various people and an AI assistant. The AI tries to be helpful, polite, honest, sophisticated, emotionally aware, and humble-but-knowledgeable. The assistant is happy to help with almost anything, and will do its best to understand exactly what is needed. It also tries to avoid giving false or misleading information, and it caveats when it isn't entirely sure about the right answer. That said, the assistant is practical and really does its best, and doesn't let caution get too much in the way of being useful.\\n\"\n",
        "\n",
        "# PROMPT = \"\"\"Use the following pieces of context to answer the question at the end.\n",
        "# If you don't know the answer, just say that you don't know, don't try to\n",
        "# make up an answer. Don't make up new terms which are not available in the context.\n",
        "# {context}\"\"\"\n",
        "\n",
        "PARAMETERS = {\n",
        "    \"temperature\": 0.9,\n",
        "    \"top_p\": 0.95,\n",
        "    \"repetition_penalty\": 1.2,\n",
        "    \"top_k\": 50,\n",
        "    \"truncate\": 1000,\n",
        "    \"max_new_tokens\": 1024,\n",
        "    \"seed\": 42,\n",
        "    \"stop_sequences\": [\"<|endoftext|>\", \"</s>\"],\n",
        "}\n",
        "\n",
        "# Initialize Hugging Face pipeline\n",
        "api_token = \"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\"\n",
        "model_name = \"openai-community/gpt2\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=api_token, trust_remote_code=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, use_auth_token=api_token, trust_remote_code=True)\n",
        "generator = pipeline('text-generation', model=model, tokenizer=tokenizer, device=0 if torch.cuda.is_available() else -1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-07-11T12:45:01.369513Z",
          "iopub.status.busy": "2024-07-11T12:45:01.368974Z",
          "iopub.status.idle": "2024-07-11T12:45:01.376460Z",
          "shell.execute_reply": "2024-07-11T12:45:01.375561Z",
          "shell.execute_reply.started": "2024-07-11T12:45:01.369479Z"
        },
        "trusted": true,
        "id": "ToHGm-kV98Fs"
      },
      "outputs": [],
      "source": [
        "# Define the preprompt and prompt\n",
        "PREPROMPT = \"Below is a conversation with an AI assistant. The assistant is helpful, polite, and knowledgeable.\\n\\n\"\n",
        "PROMPT = \"User: {query}\\nAI:\"\n",
        "\n",
        "# def generate_response(query):\n",
        "#     # Format the prompt with the user's query\n",
        "#     full_prompt = PREPROMPT + PROMPT.format(query=query)\n",
        "\n",
        "#     # Generate response\n",
        "#     response = generator(full_prompt, max_length=150, truncation=True, do_sample=True, top_p=0.95, top_k=50, num_return_sequences=2)\n",
        "#     response = generator(full_prompt, max_length=PARAMETERS[\"max_new_tokens\"], do_sample=True, temperature=PARAMETERS[\"temperature\"], top_p=PARAMETERS[\"top_p\"], repetition_penalty=PARAMETERS[\"repetition_penalty\"], top_k=PARAMETERS[\"top_k\"], truncation=True)\n",
        "\n",
        "def generate_response(query, results):\n",
        "    # Format the prompt with the user's query\n",
        "    full_prompt = PREPROMPT + PROMPT.format(query=query)\n",
        "\n",
        "    # Generate response\n",
        "    response = generator(full_prompt, max_length=150, truncation=True, do_sample=True, top_p=0.95, top_k=50, num_return_sequences=2)\n",
        "\n",
        "    # Extract the generated text\n",
        "    generated_text = response[0]['generated_text']\n",
        "\n",
        "    # Extract the part of the response that is the assistant's reply\n",
        "    ai_response = generated_text.split(\"AI:\")[1].strip() if \"AI:\" in generated_text else generated_text\n",
        "\n",
        "    # Append search results to the AI's response\n",
        "    if results:\n",
        "        ai_response += \"\\n\\nSearch Results:\\n\" + \"\\n\".join(results)\n",
        "\n",
        "    # Create a JSON response\n",
        "    json_response = {\"response\": ai_response}\n",
        "\n",
        "    return json_response\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-07-11T12:45:01.401210Z",
          "iopub.status.busy": "2024-07-11T12:45:01.400880Z",
          "iopub.status.idle": "2024-07-11T12:45:01.410386Z",
          "shell.execute_reply": "2024-07-11T12:45:01.409519Z",
          "shell.execute_reply.started": "2024-07-11T12:45:01.401180Z"
        },
        "trusted": true,
        "id": "dSYQQ7oN98Fu"
      },
      "outputs": [],
      "source": [
        "def parse_args():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--fname\", type=str, required=True)\n",
        "    parser.add_argument(\"--top_k\", type=int, default=32)\n",
        "    parser.add_argument(\"--window_size\", type=int, default=128)\n",
        "    parser.add_argument(\"--step_size\", type=int, default=100)\n",
        "\n",
        "    # Simulate command line arguments\n",
        "    args = parser.parse_args([\n",
        "        '--fname', filename,\n",
        "        '--top_k', '32',\n",
        "        '--window_size', '128',\n",
        "        '--step_size', '100'\n",
        "    ])\n",
        "\n",
        "    return args"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-07-11T12:45:01.411987Z",
          "iopub.status.busy": "2024-07-11T12:45:01.411580Z",
          "iopub.status.idle": "2024-07-11T12:45:01.420031Z",
          "shell.execute_reply": "2024-07-11T12:45:01.419181Z",
          "shell.execute_reply.started": "2024-07-11T12:45:01.411956Z"
        },
        "trusted": true,
        "id": "Xu4gOUM498Fv"
      },
      "outputs": [],
      "source": [
        "def embed(fname, window_size, step_size):\n",
        "    text = extract_text(fname)\n",
        "    text = \" \".join(text.split())\n",
        "    text_tokens = text.split()\n",
        "\n",
        "    sentences = []\n",
        "    for i in range(0, len(text_tokens), step_size):\n",
        "        window = text_tokens[i : i + window_size]\n",
        "        if len(window) < window_size:\n",
        "            break;\n",
        "        sentences.append(window)\n",
        "\n",
        "    paragraphs = [\" \".join(s) for s in sentences]\n",
        "#     print(paragraphs)\n",
        "\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    model = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\", device=device)\n",
        "\n",
        "    model.max_seq_length = 512\n",
        "    cross_encoder = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\", device=device)\n",
        "\n",
        "    embeddings = model.encode(paragraphs, show_progress_bar=True, convert_to_tensor=True)\n",
        "\n",
        "    return model, cross_encoder, embeddings, paragraphs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-07-11T12:45:01.421554Z",
          "iopub.status.busy": "2024-07-11T12:45:01.421275Z",
          "iopub.status.idle": "2024-07-11T12:45:01.429596Z",
          "shell.execute_reply": "2024-07-11T12:45:01.428613Z",
          "shell.execute_reply.started": "2024-07-11T12:45:01.421532Z"
        },
        "trusted": true,
        "id": "tdm0NNQP98Fv"
      },
      "outputs": [],
      "source": [
        "def search(query, model, cross_encoder, embeddings, paragraphs, top_k):\n",
        "    query_embeddings = model.encode(query, convert_to_tensor=True)\n",
        "    query_embeddings = query_embeddings.cuda() if torch.cuda.is_available() else query_embeddings\n",
        "    hits = util.semantic_search(\n",
        "        query_embeddings,\n",
        "        embeddings,\n",
        "        top_k=top_k,\n",
        "    )[0]\n",
        "\n",
        "    cross_inputs = [[query, paragraphs[hit[\"corpus_id\"]]] for hit in hits]\n",
        "#     print(\"Cross Inputs:\", cross_inputs)  # Add this line to inspect 'cross_inputs'\n",
        "\n",
        "    if not cross_inputs:\n",
        "        return []\n",
        "\n",
        "    cross_scores = cross_encoder.predict(cross_inputs)\n",
        "    for idx in range(len(cross_scores)):\n",
        "        hits[idx][\"cross_score\"] = cross_scores[idx]\n",
        "\n",
        "    results = []\n",
        "    hits = sorted(hits, key=lambda x: x[\"cross_score\"], reverse=True)\n",
        "    for hit in hits[:5]:\n",
        "        results.append(paragraphs[hit[\"corpus_id\"]].replace(\"\\n\", \" \"))\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# if __name__ == \"__main__\":\n",
        "#     args = parse_args()\n",
        "#     model, cross_encoder, embeddings, paragraphs = embed(\n",
        "#         args.fname,\n",
        "#         args.window_size,\n",
        "#         args.step_size,\n",
        "#     )\n",
        "#     print(embeddings.shape)\n",
        "#     while True:\n",
        "#         print(\"\\n\")\n",
        "#         query = input(\"Please enter your query:\")\n",
        "#         results = search(\n",
        "#             query,\n",
        "#             model,\n",
        "#             cross_encoder,\n",
        "#             embeddings,\n",
        "#             paragraphs,\n",
        "#             top_k=args.top_k,\n",
        "#         )\n",
        "# #         for result in results:\n",
        "# #             print(results)\n",
        "\n",
        "#         ai_response = generate_response(query, results)\n",
        "#         print(\"AI Chabot:\", ai_response)\n",
        "# #         print (jsonify({\"AI response\": ai_response }))"
      ],
      "metadata": {
        "id": "1nkZljPWBi7q"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-07-11T12:46:17.422997Z",
          "iopub.status.busy": "2024-07-11T12:46:17.422602Z",
          "iopub.status.idle": "2024-07-11T12:46:23.714011Z",
          "shell.execute_reply": "2024-07-11T12:46:23.713047Z",
          "shell.execute_reply.started": "2024-07-11T12:46:17.422962Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 626
        },
        "id": "4OsT2UeF98Fw",
        "outputId": "f862cbbc-0926-463f-dcaa-abc6da2c3a78"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://ffbcf459a7ac57742e.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://ffbcf459a7ac57742e.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import gradio as gr\n",
        "def chatbot_interface(query):\n",
        "    # Simulate the embedding process and search\n",
        "    args = parse_args()\n",
        "    model, cross_encoder, embeddings, paragraphs = embed(\n",
        "        args.fname,\n",
        "        args.window_size,\n",
        "        args.step_size,\n",
        "    )\n",
        "\n",
        "    # Get search results\n",
        "    search_results = search(\n",
        "        query,\n",
        "        model,\n",
        "        cross_encoder,\n",
        "        embeddings,\n",
        "        paragraphs,\n",
        "        top_k=args.top_k,\n",
        "    )\n",
        "    ai_response = generate_response(query, search_results)\n",
        "    print(\"AI Chabot:\", ai_response)\n",
        "    return ai_response\n",
        "#     print (jsonify({\"AI response\": ai_response }))\n",
        "\n",
        "# Create Gradio interface\n",
        "iface = gr.Interface(\n",
        "    fn=chatbot_interface,\n",
        "    inputs='text',\n",
        "    outputs='text',\n",
        "    title=\"AI Chatbot\",\n",
        "    description=\"Enter your query and get a response from the AI chatbot.\"\n",
        ")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    iface.launch()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-11T12:45:01.482646Z",
          "iopub.status.idle": "2024-07-11T12:45:01.483088Z",
          "shell.execute_reply": "2024-07-11T12:45:01.482875Z",
          "shell.execute_reply.started": "2024-07-11T12:45:01.482857Z"
        },
        "trusted": true,
        "id": "g7zcJNeX98Fx"
      },
      "outputs": [],
      "source": [
        "# if __name__ == \"__main__\":\n",
        "#     args = parse_args()\n",
        "#     model, cross_encoder, embeddings, paragraphs = embed(\n",
        "#         args.fname,\n",
        "#         args.window_size,\n",
        "#         args.step_size,\n",
        "#     )\n",
        "#     print(embeddings.shape)\n",
        "#     while True:\n",
        "#         print(\"\\n\")\n",
        "#         query = input(\"Please enter your query:\")\n",
        "#         results = search(\n",
        "#             query,\n",
        "#             model,\n",
        "#             cross_encoder,\n",
        "#             embeddings,\n",
        "#             paragraphs,\n",
        "#             top_k=args.top_k,\n",
        "#         )\n",
        "# #         for result in results:\n",
        "# #             print(results)\n",
        "\n",
        "#         ai_response = generate_response(query, results)\n",
        "#         print(\"AI Chabot:\", ai_response)\n",
        "# #         print (jsonify({\"AI response\": ai_response }))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-11T12:45:01.484642Z",
          "iopub.status.idle": "2024-07-11T12:45:01.485009Z",
          "shell.execute_reply": "2024-07-11T12:45:01.484860Z",
          "shell.execute_reply.started": "2024-07-11T12:45:01.484846Z"
        },
        "trusted": true,
        "id": "iicieQKZ98Fx"
      },
      "outputs": [],
      "source": [
        "# from flask import Flask, request, jsonify\n",
        "# app = Flask(__name__)\n",
        "# @app.route('/chatbot', methods=['POST'])\n",
        "# def chat():\n",
        "#     data = request.json\n",
        "#     query = data.get(\"query\")\n",
        "#     if not query:\n",
        "#         return jsonify({\"error\": \"No query provided\"}), 400\n",
        "\n",
        "#     response = chatbot_interface(query)\n",
        "#     return jsonify({\"AI response\": response })\n",
        "\n",
        "# if __name__ == '__main__':\n",
        "#     app.run(port=5000)"
      ]
    }
  ],
  "metadata": {
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "datasetId": 5348920,
          "sourceId": 8895605,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30733,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}